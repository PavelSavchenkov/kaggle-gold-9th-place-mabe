{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9119d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from common.config_utils import base_model_from_file, base_model_to_file\n",
    "from gbdt.helpers import get_any_train_config, get_train_config\n",
    "from postprocess.submission_utils import Submission, copy_model_file_to_submission\n",
    "\n",
    "dst_submission_dir = Path(\"submissions\") / \"full-models-26-nov-fix\"\n",
    "if dst_submission_dir.exists():\n",
    "    shutil.rmtree(dst_submission_dir)\n",
    "dst_submission_dir.mkdir(exist_ok=True)\n",
    "\n",
    "submission = base_model_from_file(\n",
    "    Submission, \"submissions/e-nested-f1-all/submission.json\"\n",
    ")\n",
    "\n",
    "mode = \"same\"\n",
    "# mode = \"max\"\n",
    "# mode = \"median\"\n",
    "\n",
    "new_models = []\n",
    "for model in tqdm(submission.models):\n",
    "    config_old = get_any_train_config(model.name)\n",
    "    # if config.group != model.name:\n",
    "    #     print(f\"model.name={model.name}, config.group={config.group}\")\n",
    "    new_model_name = f\"{config_old.group}-full\"\n",
    "    dst = dst_submission_dir / new_model_name\n",
    "    suff = \"\"\n",
    "    if (Path(\"train_logs\") / f\"{new_model_name}-seed0-fix\").exists():\n",
    "        suff = \"-fix\"\n",
    "    config_new = get_any_train_config(f\"{new_model_name}-seed0{suff}\")\n",
    "    assert config_new.get_num_trees() >= max(model.steps)\n",
    "    for seed in range(5):\n",
    "        full_name = f\"{new_model_name}-seed{seed}{suff}\"\n",
    "        full_path = Path(\"train_logs\") / full_name\n",
    "        if not full_path.exists():\n",
    "            print(f\"FAIL at {full_name}\")\n",
    "            continue\n",
    "        dst_ckpt_path = dst / f\"cv{seed}\"\n",
    "        if not dst_ckpt_path.exists():\n",
    "            shutil.copytree(full_path / \"cv0\", dst_ckpt_path)\n",
    "            for dir in dst_ckpt_path.rglob(\"*\"):\n",
    "                if dir.is_dir() and dir.name == \"test\":\n",
    "                    shutil.rmtree(dir)\n",
    "\n",
    "    # keep same set of steps\n",
    "    model.name = new_model_name\n",
    "    if mode == \"same\":\n",
    "        pass\n",
    "    elif mode == \"max\":\n",
    "        step = max(model.steps)\n",
    "        model.steps = [step] * len(model.steps)\n",
    "    elif mode == \"median\":\n",
    "        step = int(np.median(model.steps))\n",
    "        model.steps = [step] * len(model.steps)\n",
    "    config = get_train_config(f\"{new_model_name}-seed0{suff}\", 0)\n",
    "    num_trees = config.get_num_trees()\n",
    "    steps = []\n",
    "    for step in model.steps:\n",
    "        if step > num_trees:\n",
    "            print(f\"PROBLEM with {config.group}\")\n",
    "            print(f\"step: {step}, num_trees in full model: {num_trees}, max step was {max(model.steps)}, steps were {model.steps}\")\n",
    "            print()\n",
    "        step = min(step, num_trees - 1)\n",
    "        steps.append(step)\n",
    "    model.steps = steps\n",
    "    new_models.append(model)\n",
    "submission.models = new_models\n",
    "base_model_to_file(submission, f\"submission_full_{mode}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gbdt.helpers import get_train_config\n",
    "import json\n",
    "\n",
    "per_action = {}\n",
    "for model_path in Path(\"train_logs\").iterdir():\n",
    "    name = model_path.name\n",
    "    for cv_dir in model_path.iterdir():\n",
    "        if not cv_dir.is_dir():\n",
    "            continue\n",
    "        cv = cv_dir.name\n",
    "        imps = cv_dir / \"final_model\" / \"feature_importances\" / \"overall.txt\"\n",
    "        if imps.exists():\n",
    "            config_old = get_train_config(name, cv)\n",
    "            if config_old.action in per_action:\n",
    "                continue\n",
    "            content = map(lambda s: s.strip(), open(imps).readlines()[:50])\n",
    "            per_action[config_old.action] = \"\\n\".join(content)\n",
    "json.dump(per_action, open(\"all_importances.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from common.constants import ACTION_NAMES_IN_TEST\n",
    "from common.ensemble_building_primitives import EnsembleObjective\n",
    "from config_utils import base_model_to_str\n",
    "from gbdt.rebalance_utils import DurationStats\n",
    "from postprocess.ensemble_utils import EnsembleApproach\n",
    "\n",
    "\n",
    "def get_action_data(dir: Path, action: str) -> str | None:\n",
    "    json_path = dir / f\"{action}.json\"\n",
    "    if not json_path.exists():\n",
    "        return None\n",
    "    f1_map = json.load(open(json_path))\n",
    "\n",
    "    row_per_app = []\n",
    "    for app in f1_map[\"f1_valid\"].keys():\n",
    "        f1_valid = f1_map[\"f1_valid\"][app]\n",
    "        # f1_train = f1_map[\"f1_train\"][app]\n",
    "        row_per_app.append((f1_valid, f\"f1_valid={f1_valid:.5f}, App: {app}\"))\n",
    "    row_per_app.sort(reverse=True) \n",
    "\n",
    "    desc = f\"action={action}\\n\"\n",
    "    for (f1_valid, s) in row_per_app:\n",
    "        desc += s + \"\\n\"\n",
    "    desc += \"\\n\"\n",
    "    return desc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for action in ACTION_NAMES_IN_TEST:\n",
    "    # desc = get_action_data(Path(\"approaches\"), action)\n",
    "    # if desc is None:\n",
    "    #     continue\n",
    "    # print(desc)\n",
    "    # continue\n",
    "\n",
    "    desc_per_action = get_action_data(Path(\"e_apps/approaches_per_action\"), action)\n",
    "    desc_logloss = get_action_data(Path(\"e_apps/approaches_logloss\"), action)\n",
    "    if desc_per_action is None or desc_logloss is None:\n",
    "        continue\n",
    "    print(f\"PR-AUC CKPT SELECTION:\")\n",
    "    print(desc_per_action)\n",
    "    print(f\"LOGLOSS CKPT SELECTION:\")\n",
    "    print(desc_logloss)\n",
    "    print(\"-----------\")\n",
    "# logloss ckpt, nested-f1 approach:\n",
    "# chase, chaseattack, huddle, intromit, mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e15007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cnt paths: 71\n",
      "True split_cache/fold_id_61526ff9937bbcd3ae1a6f7d798e041d3f29df58be50138ab9a03d004fa88ac0.npy allogroom\n",
      "True split_cache/fold_id_931f537df70aca17baf487cb4c69ae72a5cc19fb162fee0312d32accd8075c8d.npy approach\n",
      "True split_cache/fold_id_e38878aa3d1cfcd6c13bb518ba05be8eef1d15340032365f4338afe6cc49c810.npy attack\n",
      "True split_cache/fold_id_bc7f447d7fd01e5bfd5b262ff11481be2f3b7123b51c50ca6856223b6148d985.npy attemptmount\n",
      "True split_cache/fold_id_545c5635583db71b5b8ac74ef7e5c5f958d57efb276343c19c961eb291409e33.npy avoid\n",
      "True split_cache/fold_id_1744ec9954b4a604e15a028bd392d2e930a66fe4373bd3639be1d9ea7bba2ce8.npy biteobject\n",
      "True split_cache/fold_id_795da20bcc92b50023625958b6e52a7c14abed22afb187809f6de944a2e4f699.npy chase\n",
      "True split_cache/fold_id_054bdd5a71ca01827810ff257bfd0a4b8f0374986956965645bb5c9c4aaac47e.npy chaseattack\n",
      "True split_cache/fold_id_9abe77f707dc2145cd0b99bdce21e3e47f3a53be167f510328f18143947ec761.npy climb\n",
      "True split_cache/fold_id_0607428e9a9fd69843c5c343e4b5b1abbbe9275cf34ad589f3923b1ccfbf254c.npy defend\n",
      "True split_cache/fold_id_249fa2db41e308d71873f6cc4ed1f57238d2f678bd34a476644f615f93417a26.npy dig\n",
      "True split_cache/fold_id_be496f58d63a9bbd9ce03139b8adf0d6929cc80815ae881a8b67296afb9b2c67.npy dominance\n",
      "True split_cache/fold_id_6b0f8d1a7eb417d588bef0f46698edaf5911ad5235a492b5a3b444e5f6e3e141.npy dominancegroom\n",
      "True split_cache/fold_id_51b8de13970ba3e45fc23f74130cfd258d603473274a9f59907364f347351d50.npy escape\n",
      "True split_cache/fold_id_18d37102f651cbc673419ef0a0ef73bd27d41d0583cf55a2580d9c9c957217f0.npy exploreobject\n",
      "True split_cache/fold_id_3c3a495180554ba268e0a6fde9f03d6f78b0899be9d2efa84c6cbf99193eed17.npy flinch\n",
      "True split_cache/fold_id_883a135e937830a82db356233ae6a8660e1c29d30e1c027e7d151a0894ddeefa.npy follow\n",
      "True split_cache/fold_id_b05666cac8f5bafeed77f3965b4cb4364de91b0c16b1efff6945692d414af3de.npy freeze\n",
      "True split_cache/fold_id_b77658e870117afe32040ed0e123a923408f5af2ba6893e78456bacc6bfc3f38.npy huddle\n",
      "True split_cache/fold_id_7b425a07092b983cb18f177e5e888df9f5d8cb352b54991350c9808710381c52.npy intromit\n",
      "True split_cache/fold_id_c7ea1ad3152fc687b4f03f69d07bcc409adab702899f998a0e809a49f3f1b732.npy mount\n",
      "True split_cache/fold_id_e2438661990f5b2474d67a61f3d62df8cbb5b73bcb4f9fe60b32c044e688f9b2.npy rear\n",
      "True split_cache/fold_id_4ea5d73b43a455f39243ec0709ee783ac19c1d18d3bbeb72eed736ce04c5f8e9.npy reciprocalsniff\n",
      "True split_cache/fold_id_8a7c6c269891722a01afe15458398b72cfa532e414f89d10d44df4fe52100c64.npy rest\n",
      "True split_cache/fold_id_d3e3fa949672917ff1ee336afc5176d34439a3014e9bd392feb1773a372b3d41.npy run\n",
      "True split_cache/fold_id_ff93660543598f67193b23f55bf0f6efb369b838036570aa650584d8bc69edbf.npy selfgroom\n",
      "True split_cache/fold_id_01d2361585a0c011242b3da572b3b8b2971435c02f364f6e72b2d305326a10c7.npy shepherd\n",
      "True split_cache/fold_id_5c31e841e7a5c2ffd29e53a6febb4698db3b0a2c6fc31975887c3d62fa79b806.npy sniff\n",
      "True split_cache/fold_id_93158eae434662e465251718cf30613385d06ca889f3a1c868f29b99d744d50d.npy sniffbody\n",
      "True split_cache/fold_id_4691ec321971a835d0114fc58eb28e58bae316cb876e8c404b88cbabc4bd3955.npy sniffface\n",
      "True split_cache/fold_id_97aef908420d9ea539f52edd274c90e83308cd4aeb5f736c5b4d5e7b5fddab42.npy sniffgenital\n",
      "True split_cache/fold_id_8fed5a5802484dd975201e470a446e5b7c4c185558d434223765c910adc3e21d.npy submit\n",
      "True split_cache/fold_id_568c9869ee7e7fbd7d0acebd14b57341b31237fffdb4dc3daa1db1e87453f019.npy tussle\n",
      "True split_cache/fold_id_61526ff9937bbcd3ae1a6f7d798e041d3f29df58be50138ab9a03d004fa88ac0.npy allogroom\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "PosixPath('split_cache/fold_id_61526ff9937bbcd3ae1a6f7d798e041d3f29df58be50138ab9a03d004fa88ac0.npy')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(cache_path.exists(), cache_path, action)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[43mall_paths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     fold_id = np.load(cache_path)\n\u001b[32m     88\u001b[39m     by_action[action].append(fold_id)\n",
      "\u001b[31mKeyError\u001b[39m: PosixPath('split_cache/fold_id_61526ff9937bbcd3ae1a6f7d798e041d3f29df58be50138ab9a03d004fa88ac0.npy')"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from common.constants import ACTION_NAMES_IN_TEST, LAB_NAMES_IN_TEST\n",
    "from common.helpers import get_train_meta\n",
    "from common.parse_utils import parse_behaviors_labeled\n",
    "from common.config_utils import DataSplitConfig\n",
    "from common.folds_split_utils import _fold_cache_key\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from gbdt.helpers import is_fully_trained\n",
    "\n",
    "# def calc_fold_id(\n",
    "#     meta: pd.DataFrame,\n",
    "#     config: DataSplitConfig,\n",
    "#     cache_dir: Path | str,\n",
    "#     force_recalc: bool = False,\n",
    "# ):\n",
    "#     cache_path: Optional[Path] = None\n",
    "#     if cache_dir:\n",
    "#         cache_dir = Path(cache_dir)\n",
    "#         cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "#         cache_key = _fold_cache_key(meta=meta, config=config)\n",
    "#         cache_path = cache_dir / f\"fold_id_{cache_key}.npy\"\n",
    "#         if cache_path.exists() and not force_recalc:\n",
    "#             cached = np.load(cache_path, allow_pickle=False)\n",
    "#             return cached.astype(int)\n",
    "\n",
    "# train_configs = []\n",
    "# for model_path in Path(\"train_logs\").iterdir():\n",
    "#     name = model_path.name\n",
    "#     train_config = get_any_train_config(name)\n",
    "#     if train_config:\n",
    "#         train_configs.append(train_config)\n",
    "\n",
    "cache_dir = Path(\"split_cache\")\n",
    "\n",
    "all_paths = set()\n",
    "for path in cache_dir.iterdir():\n",
    "    all_paths.add(path)\n",
    "\n",
    "print(f\"Cnt paths: {len(all_paths)}\")\n",
    "by_action = defaultdict(list)\n",
    "train_meta = get_train_meta()\n",
    "for action in ACTION_NAMES_IN_TEST:\n",
    "    config_old = DataSplitConfig(seed=0, num_folds=5, test_fold=0, train_folds=None, actions=[action])\n",
    "\n",
    "    should_keep_video = train_meta[\"behaviors_labeled\"].apply(\n",
    "        lambda beh: any(\n",
    "            item.action in config_old.actions for item in parse_behaviors_labeled(beh)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    meta = train_meta[should_keep_video]\n",
    "\n",
    "    always_train_mask = ~meta[\"lab_id\"].isin(LAB_NAMES_IN_TEST)\n",
    "    always_train_rows = meta[always_train_mask].copy()\n",
    "    meta = meta[~always_train_mask]\n",
    "\n",
    "    cache_key = _fold_cache_key(meta=meta, config=config_old)\n",
    "    cache_path = cache_dir / f\"fold_id_{cache_key}.npy\"\n",
    "    print(cache_path.exists(), cache_path, action)\n",
    "    if cache_path.exists():\n",
    "        all_paths.remove(cache_path)\n",
    "        fold_id = np.load(cache_path)\n",
    "        by_action[action].append(fold_id)\n",
    "\n",
    "for action in ACTION_NAMES_IN_TEST:\n",
    "    config_old = DataSplitConfig(seed=0, num_folds=5, test_fold=0, train_folds=None, actions=[action])\n",
    "\n",
    "    should_keep_video = train_meta[\"behaviors_labeled\"].apply(\n",
    "        lambda beh: any(\n",
    "            item.action in config_old.actions for item in parse_behaviors_labeled(beh)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    meta = train_meta[should_keep_video]\n",
    "\n",
    "    always_train_mask = ~meta[\"lab_id\"].isin(LAB_NAMES_IN_TEST)\n",
    "    always_train_rows = meta[always_train_mask].copy()\n",
    "    meta = meta[~always_train_mask]\n",
    "\n",
    "    del config_old.train_folds\n",
    "    cache_key = _fold_cache_key(meta=meta, config=config_old)\n",
    "    cache_path = cache_dir / f\"fold_id_{cache_key}.npy\"\n",
    "    print(cache_path.exists(), cache_path, action)\n",
    "    if cache_path.exists():\n",
    "        all_paths.remove(cache_path)\n",
    "        fold_id = np.load(cache_path)\n",
    "        by_action[action].append(fold_id)\n",
    "\n",
    "print(f\"Remain: {len(all_paths)}\")\n",
    "\n",
    "assert len(by_action.keys()) == len(ACTION_NAMES_IN_TEST)\n",
    "for action in by_action.keys():\n",
    "    print(f\"action={action}, cnt caches: {len(by_action[action])}\")\n",
    "    fold_id = by_action[action][0]\n",
    "    for i in range(1, len(by_action[action])):\n",
    "        fold_id_another = by_action[action][i]\n",
    "        assert np.all(fold_id == fold_id_another)\n",
    "        print(f\"{id(fold_id)} == {id(fold_id_another)}\")\n",
    "\n",
    "for path in all_paths:\n",
    "    fold_id = np.load(path)\n",
    "    found = False\n",
    "    for action, fs in by_action.items():\n",
    "        for f in fs:\n",
    "            if f.shape == fold_id.shape and np.all(f == fold_id):\n",
    "                print(f\"Found action={action} for {path}\")\n",
    "                found = True\n",
    "    if not found:\n",
    "        print(f\"Nothing for {path}, fold_id: {fold_id.shape}, {fold_id.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
